$schema: ../../../app/schemas/content-page/content-page.schema.json
title: HRA Organ Gallery
subtitle: Explore organs, donor data, tissue blocks, datasets, and cells across scales in virtual reality.
action:
  label: Use app
  url: https://www.meta.com/experiences/hra-organ-gallery/5696814507101529/
content:
  - component: PageSection
    tagline: Overview
    anchor: overview
    level: 2
    content:
      component: Markdown
      data: |
        The Human Reference Atlas (HRA) Organ Gallery is a virtual reality (VR) application that enables users to explore 3D organ models of the HRA in
        their true size, location, and spatial relation to each other.
        The HRA Organ Gallery has two main use cases: 1) introducing both novice and expert users to the 2D and 3D data available in the HRA via the HuBMAP Data Portal,
        the SenNet Data Portal, and similar efforts, and 2) providing quality assurance and quality control (QA/QC) for HRA data providers.
        More use cases are under development.
        The concept of the application is described in this [publication](https://www.frontiersin.org/journals/bioinformatics/articles/10.3389/fbinf.2023.1162723/full).
        Figure 1 compares the user interface for exploring the HRA via the [Exploration User Interface (EUI)](https://apps.humanatlas.io/eui/) to how it appears in the
        HRA Organ Gallery when using a Meta Quest 2 or 3.
        This research is based on work supported by a [CIFAR](https://cifar.ca/research-programs/cifar-macmillan-multiscale-human/) catalyst award.

  - component: PageSection
    tagline: 2D vs. virtual reality
    anchor: 2d-virtual-reality
    level: 3
    content:
      - component: Markdown
        data: |
          **(A)**: A user looks at the HRA with the [Exploration User Interface](https://apps.humanatlas.io/eui/) in a standard-size browser window on a 17-in display.
          **(B)**: The HRA Organ Gallery allows the user to view the organs, tissue blocks, and cell type counts of the HRA in true scale using immersive technology (VR).
          [Source](https://www.biorxiv.org/content/10.1101/2023.02.13.528002v1).
          More information can be found at our [README](https://github.com/cns-iu/hra-organ-gallery-in-vr/blob/main/README.md).<br><br>

          <img src="assets/content/organ-gallery-page/images/organ_gallery.png" width="100%" />

  - component: PageSection
    tagline: Background
    anchor: background
    level: 3
    content:
      - component: Markdown
        data: |
          The HRA, funded by HuBMAP, SenNet, and similar efforts, aims to map the adult healthy human body at single-cell resolution through a collaboration across 17
          (and counting) international consortia. The project includes three main categories of data: biological structure, spatial, and specimen data.
          The ASCT+B tables are compiled by experts to capture biological structure data, describing the connection between anatomical structures, cell types, and biomarkers.

  - component: PageSection
    tagline: Why virtual reality?
    anchor: why-virtual-reality
    level: 4
    content:
      component: Markdown
      data: |
        Using a visually explicit method of data integration, the HRA enables users to explore disparate data. Virtual reality (VR) presents
        a unique opportunity to explore both spatial and abstract data in an immersive environment that enhances presence beyond traditional
        interfaces like windows, icons, menus, and pointers, also called WIMP paradigm (Van Dam, 1997).
        Although some users may be able to learn how to explore 3D reference organs and tissue blocks on a 2D screen (Bueckle et al., 2021, 2022),
        many still struggle with interacting with 3D objects on a 2D screen.

  - component: PageSection
    tagline: Data visualizations
    anchor: data-visualizations
    level: 4
    content:
      component: Markdown
      data: |
        The application's primary building blocks, presented in Figure 3 of [the preprint paper](https://www.biorxiv.org/content/10.1101/2023.02.13.528002v1),
        include the SceneBuilder which serves as the data manager for the application and retrieves data from the HRA API, and the DataLoader which uses Node,
        NodeArray, and GLBObject classes to store 3D organs in GLB format.
        Once the setup is complete, the user can interact with the organs, and the entire scene takes about 5-7 seconds to load when running natively on the Meta Quest 2 or 3.
        Through the HRA Organ Gallery, users can investigate 55 3D reference organs and 800+ mapped tissue blocks obtained from 300+ diverse donors and providers and
        20+ tissue data providers (as of HRA v2.1), connected to 6000+ datasets, in a cohesive, immersive, and 3D VR environment at the convergence of VR, information visualization,
        and bioinformatics.

  - component: PageSection
    tagline: Feedback
    anchor: feedback
    level: 2
    content:
      component: Markdown
      data: |
        Information for test users is available [here](https://github.com/cns-iu/hra-organ-gallery-in-vr/blob/main/INFORMATION_FOR_TESTERS.MD).
        In order to engage a diverse group of users in the development of the HRA Organ Gallery,
        we request feedback from a broad range of specialists in fields such as bioinformatics, 3D modeling, medical illustration, anatomy, data curation, and biology.
        We are continuous evaluating the app's usability, engagement, and presence.

        Also, please email Andreas Bueckle (abueckle@iu.edu) if you are interested in contributing your input to the research and development of the HRA Organ Gallery.

  - component: PageSection
    tagline: References
    anchor: references
    level: 2
    content:
      component: Markdown
      data: |
        A. Bueckle, C. Qing, S. Luley, Y. Kumar, N. Pandey, and K. Börner, “The HRA Organ Gallery affords immersive superpowers for building and explorring the
        Human Reference Atlas with virtual reality,” *Frontiers in Bioinformatics*, vol. 3, 2023. doi: [10.3389/fbinf.2023.1162723](https://www.frontiersin.org/journals/bioinformatics/articles/10.3389/fbinf.2023.1162723/full).

        Bueckle, Andreas., Kilian Buehling, Patrick C. Shih, and Katy Börner. 2022. ["Optimizing Performance and Satisfaction in Matching and Movement Tasks in
        Virtual Reality with Interventions Using the Data Visualization Literacy Framework"](https://www.frontiersin.org/journals/virtual-reality/articles/10.3389/frvir.2021.727344/full).
        *Frontiers in Virtual Reality* 2. doi: 10.3389/frvir.2021.727344.

        Bueckle, Andreas., Kilian Buehling, Patrick C. Shih, and Katy Börner. 2021. ["3D Virtual Reality vs. 2D Desktop Registration User Interface Comparison"](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0258103).
        *PLOS ONE* 16 (10): e0258103. doi: 10.1371/journal.pone.0258103.

        van Dam, Andries. 1997. ["Post-WIMP User Interfaces"](https://dl.acm.org/doi/10.1145/253671.253708).
        *Communications of the ACM 40* (2): 63-67. doi: 10.1145/253671.253708.
